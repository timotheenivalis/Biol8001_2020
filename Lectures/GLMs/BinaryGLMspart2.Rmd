---
title: "Binary GLMs part 2"
author: "Timothee Bonnet"
date: "4 September 2020"
output: 
  html_document:
    theme: united
    highlight: pygments
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggbeeswarm)
```


The data set "voles_early.csv" contains records from the begining of the survey of a wild rodent population. Among the variables is "survival", a binary variable indicating whether an individual captured on a given year survived to the next year. We want to understand variation in survival and in particular whether there is natural selection on body mass through survival. Other variables than body mass probably structure the variation in survival, so we want to do some exploration before rushing to model survival as a function of body mass.

![cute snow vole](cutevole.jpg)

Let's start by considering whether sex explains variation in survival. 

```{r}
survdat_early <- read_csv("../../Data/voles_early.csv")
str(survdat_early)

survdat_early %>% group_by(sex, survival) %>%
  summarize(count=n()) %>% 
  ggplot(aes(x = sex, y=survival, size=count)) +
  geom_point()

vole_s_glm <- glm(survival ~ sex, data = survdat_early, family = "binomial")
summary(vole_s_glm)
```

How to interpret the summary output? What are these parameters and how do they relate to data?

```{r}
coef(vole_s_glm)

survdat_early %>% group_by(sex ) %>% summarize(mean=mean(survival)) 
```

Parameter estimates actually predict mean sex-specific survival, but it is not obvious because the GLM is fitted, not a the scale of probabilities, but on a transformed scale, the logit scale.

All GLMs have such a transformation. It is called the "*Link function*".

To go from a probability to the regression scale our GLM applied a logit link function:

$$
\mathrm{logit}(y) = \log(\frac{y}{1-y})
$$

To go from model predictions on the regression scale to the data scale we apply the inverse link function, which is
$$
\mathrm{logit}^{-1}(y) = \frac{1}{1+e^{-y}}
$$
in R you can run this inverse logit function with `plogis`.

So, our model told us that the predicted survival for females (the intercept) was:
```{r}
1/(1+exp(-coef(vole_s_glm)[1]))
plogis(coef(vole_s_glm)[1])
```

And the predicted survival for males was:
```{r}
plogis(coef(vole_s_glm)[1] + coef(vole_s_glm)[2] )

```

That is a rather general recipe for GLMs. When you want to calculate something on the scale of the data (that makes more sense to the human brain) from parameter estimates you:

1. Calculate the prediction using parameter estimates
2. Apply the inverse transformation used in the GLM. 

It does not work if you apply the inverse transformation on different parameters separately. 


#### Using `predict()`

It is a good idea to know how to back-transform link functions by hand: it forces you to understand how GLMs work, it is useulf to simulate data and sometimes you really need to do calculations by hand. However, most of the time functions like `predict()` are a useful shortcut. You need to be control the argument `type = ` (what scale do you want the prediction on? "link" means the scale of the linear model; "response" means scale of probability) and `newdata = ` (what predictor values do you want predictions for? If you do not use this argument you get a prediction for each real data point in your data).

```{r}
newdata_vole_sex <- tibble(sex=unique(survdat_early$sex))

predict(vole_s_glm, type = "link", newdata = newdata_vole_sex)

predict(vole_s_glm, type = "response", newdata = newdata_vole_sex)
```


#### What about data generation?

So, a GLM predicts probabilities, values between 0 and 1. But we observe only 0s and 1s. Does the GLM knows about that? Yes it does! The GLM relates probabilities to data in a very obvious way: for a predicted probability $p$, the GLM thinks you should observe a proportion $p$ of 1s and a proportion $1-p$ of 0s.
If it seems trivial, it is because it is. GLMs for binary data are really easy.

Other GLMs use more complex processes, so let us look more formally at how a binary GLM sees the world.

The distribution turning a probability $p$ into 0s and 1s is the Bernouilli distribution, a special case of the binomial distribution. We can draw random samples from the Bernouilli distribution with `rbinom(n=, size=1, prob=)`

```{r}
(bernouilli_sample <- rbinom(n = 1000, size = 1, prob = 0.3))

mean(bernouilli_sample)
```

All GLMs use distributions that have some kind of relationship between their mean and their variance. For the Bernouilli, the relationship is $variance = mean*(1-mean)$, and the mean is expected to be equal to the probability.


```{r}
var(bernouilli_sample)

mean(bernouilli_sample)*(1-mean(bernouilli_sample))
```

That means that binary data are most variable for a probability of 0.5, and least variable for probabilities close to 0 or close to 1.

```{r}
nb_rows <- 1000
variability_bernouilli <- tibble(lm_value= seq(-5,5, length.out = nb_rows),
       probability = plogis(lm_value),
       observation = rbinom(n = nb_rows, size = 1, prob = probability))

ggplot(variability_bernouilli, aes(x=lm_value, y=probability, col=probability)) +
  geom_line()+
  geom_point(inherit.aes = FALSE, aes(x=lm_value, y=observation))
```


### What a GLM is

From previous examples we saw what a GLM is:

1. A linear model (reponse = intercept + slope × predictor . . . ), what you see with summary(glm1)
2. A "Link function" = a map between the linear function (−∞ to +∞) and a
probability distribution (from 0 to 1 for Bernouilli)
3. A probability distribution (Bernouilli, Binomial, Poisson. . . ) assumed to generate
the data (either 0 or 1 for Bernouilli)

![glm scales](scales.png)


### Practice

Beyond sex, we suspect survival probability varies by age. 


```{r}
survdat_early %>% group_by(sex, age) %>%
  summarise(p_survival = mean(survival)) %>%
ggplot(aes(x=interaction(sex, age), y=p_survival))+
  geom_bar(stat = "identity")
  
```


### Bonuses

#### Interlude: What happens inside `geom_smooth()`

Let's do it "by hand" with `predict()`:

```{r}
newdata <- tibble(x=seq(min(data_linear$x), max(data_linear$x), length.out = 100))
predictions <- predict(lm0, newdata = newdata, se.fit = TRUE)
newdata$y <- predictions$fit
newdata$lowCI <- predictions$fit - 1.96*predictions$se.fit
newdata$uppCI <- predictions$fit + 1.96*predictions$se.fit


ggplot(data_linear, aes(x=x, y=y))+
geom_point() +
geom_line(data = newdata, color="blue") +
geom_ribbon(data = newdata, aes(x=x, ymin=lowCI, ymax=uppCI),
inherit.aes = FALSE, fill="blue", alpha=0.2) +
geom_segment(aes(x=x, y=y, xend= x, yend=lm0$fitted.values))

```

#### Bonus! What happens inside `predict()`?

Let's do it "by hand" with matrix multiplication.
```{r}
newdata2 <- tibble(x=seq(min(data_linear$x), max(data_linear$x), length.out = 100),
                   y=0)

# The code below reproduces the result of predict(..., se.fit=TRUE)
mm <- model.matrix(terms(lm0),newdata2)
newdata2$y <- as.vector(mm %*% lm0$coefficients)
pvar1 <- diag(mm %*% tcrossprod(vcov(lm0),mm))
newdata2 <- tibble(
  newdata2,
  lowCI2 = newdata2$y-1.96*sqrt(pvar1),
  uppCI2 = newdata2$y+1.96*sqrt(pvar1)
) 

```
The key to compute confidence intervals is `vcov()`, which extracts the variance-covariance matrix of the uncertainty in parameters; the rest is mostly matrix multiplication.



