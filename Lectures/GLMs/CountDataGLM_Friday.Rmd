---
title: "CountDataGLM_Friday"
author: "Timothee Bonnet"
date: "25 September 2020"
output: html_document
---


Today:

* Understand the mean-variance relationship in count data GLMs
* Fit, evaluate and compare different models for count data
* Interpret Poisson GLM parameters


```{r, message=FALSE}
library(tidyverse)
library(ggbeeswarm)
library(janitor)
library(emmeans)
library(glmmTMB)
library(car)
library(DHARMa)
```

```{r}
gambusia <- read_csv("Gambusia.csv")
```

Yesterday, we fitted the model:

```{r}
summary(m_poisson <- glmmTMB(No.Cop_Success ~ 1 + Treatment,
                             data=gambusia, family = poisson))
```



Using simulations, we saw that this model was not a very good representation of that data. The model is not aware of all the variation in the data:

```{r}
fsimul <- function(x) {
  tibble(simul=simulate(m_poisson)) %>% 
  tabyl(simul)
  }

tibbsimul2 <- map_dfr(.x = 1:100, .f = fsimul)

```

```{r}
ggplot(gambusia, aes(x=No.Cop_Success))+geom_bar() +
  geom_jitter(data = tibbsimul2, 
             mapping = aes(x=simul, y=n, col=as.factor(simul)), alpha=0.4)

```


### Automated tests of fit: DHARMa

Running simulations like that to assess your models can be very useful and sometimes is required to understand what is wrong, whether it matters, or what to do about it otherwise.
Fortunately, for routine checks we can assess our models more directly with the package ```DHARMa```.

```{r}
library(DHARMa)
```

The function testResiduals() first simulate data from our model, looks at the distance between model predictions and simulated data (these distances are "residuals"), then we compare the simulated residuals to the real data residuals:
```{r}
testResiduals(m_poisson)
```

Here we can diagnose at least three problems:

* The model does not fit the data (KS tests)
* The model misses a lot of variation (Dispersion test)
* The model cannot predict extreme values (Outlier test)

It is exactly what we saw in our home made simulations earlier. You can use either approach. 

How to build a better model? First we need to understand what is wrong exactly in the model structure.

### The Poisson process

Our GLM assumes that the data are generated by a Poisson process. Let's simulate that process to understand it.

```{r}
dat <- tibble(x=rpois(n = 10000, lambda = 2.8))
ggplot(dat, aes(x=x))+ geom_bar()
mean(dat$x)
var(dat$x)
```

The Poisson distribution takes a single parameter, lambda ($\lambda$). That parameter controls exactly the shape of the distribution, and is equal to the expected mean and the expected variance both. 
That is a key issue. In a Poisson distribution, the expected mean predicts the variance. There is not room for any more variation. 

In biology, this lambda is unlikely to be constant. A Poisson GLM "thinks" the predictors in the model capture all the variation in lambda, which corresponds to this set of equations:

$$
\mathrm{obs} \sim Poisson(\lambda) \\
\lambda = \mathrm{exp}(y)\\
y = \alpha + \beta x
$$

That is often not sufficient. The data would probably be better represented by 

$$
\mathrm{obs} \sim Poisson(\lambda) \\
\lambda = \mathrm{exp}(y)\\
y = \alpha + \beta x + noise
$$
That is, there is unexplained variation after accounting for the model predictors.
A Poisson GLM cannot see the noise, which produces over-dispersion, and measure uncertainty using only the variation it can see. Therefore, a Poisson GLM often under-estimate uncertainty.

If you forget about the problem of over-dispersion you will find too many false positive results. For instance, with a variance about 3 times larger than the mean, we find significant results 30% of the time when there is no true effect (ideally we expect 5%).

```{r}
## Simulate data without any effect of x
pvalues <- vector(length=100)
for (i in 1:100)
{
  x <- rnorm(50)
  y <- rnorm(50)
  lambda <- exp(y)
  obs <- sapply(lambda, rpois, n=1)
  
  m0 <- glmmTMB(obs ~ x, family = "poisson")
  pvalues[i] <- summary(m0)$coefficients$cond["x", "Pr(>|z|)"]
}

mean(pvalues < 0.05)

```



### How to solve the issue of over-dispersion?

Basically, we need to give some freedom to the mean-variance relationship. There are many ways of doing that; some will work better or worse for particular datasets. There is no silver bullet. You need to try and assess how good the model is.

A first version, called "nbinom1" ("Negative-Binomial 1"), is an extension of the Poisson model in which the variance follows the equation 
$ V= m (1+\phi)$ where $\phi$ is the over-dispersion.

```{r}
summary(m_poisson_2 <- glmmTMB(No.Cop_Success ~ 1 + Treatment,
                             data=gambusia, family = nbinom1))
```

Let's evaluate this model using DHARMa and then looking at simulated distributions:
```{r}
testResiduals(m_poisson) # previous evaluation
 
testResiduals(m_poisson_2) #much better
```


```{r}
fsimul <- function(x) {
  tibble(simul=simulate(m_poisson)) %>% 
  tabyl(simul)
  }

tibbsimul2 <- map_dfr(.x = 1:100, .f = fsimul)

ggplot(gambusia, aes(x=No.Cop_Success))+geom_bar() +
  geom_jitter(data = tibbsimul2, 
             mapping = aes(x=simul, y=n, col=as.factor(simul)),
             alpha=0.4, inherit.aes = FALSE) + ggtitle("Poisson GLM")

```

```{r}
fsimul <- function(x) {
  tibble(simul=simulate(m_poisson_2)) %>% 
  tabyl(simul)
  }

tibbsimul2 <- map_dfr(.x = 1:100, .f = fsimul)

ggplot(gambusia, aes(x=No.Cop_Success))+geom_bar() +
  geom_jitter(data = tibbsimul2, 
             mapping = aes(x=simul, y=n, col=as.factor(simul)),
             alpha=0.4, inherit.aes = FALSE) + xlim(-1,12)+ ggtitle("Nbinom1 GLM")

```

In this case the "Negative Binomial 1" seems appropriate, and much better than the strict Poisson family.

**In general avoid the Poisson family.**

(unless you model over-dispersion within the Poisson family, using random effects or a secondary dispersion linear predictor; we will not cover this topic in the course. Just remember it is dangerous but not necessarily wrong if you know what you are doing!)

### Other types of count data GLMs

#### Families

In glmmTMB there are 5 basic GLM families for count data:

```{r, cache=TRUE}
summary(m_poisson <- glmmTMB(No.Cop_Success ~ 1 + Treatment, data=gambusia, family = poisson))
summary(m_genpoi <- glmmTMB(No.Cop_Success ~ 1 + Treatment, data=gambusia, family = genpois)) 
summary(m_compois <- glmmTMB(No.Cop_Success ~ 1 + Treatment, data=gambusia, family = compois))## Slow!
summary(m_nbinom1 <- glmmTMB(No.Cop_Success ~ 1 + Treatment, data=gambusia, family = nbinom1))
summary(m_nbinom2 <- glmmTMB(No.Cop_Success ~ 1 + Treatment, data=gambusia, family = nbinom2))

```

All of them return (almost exactly) the same parameter estimates: intercept = 0.52 and TreatmentWinner=0.16. However, they return different Standard Errors, z values and p values (the ``` poisson``` family being very different from the other ones), because they have different ideas about how variation in the data works and therefore about how sure we should be about what patterns we see.

Each family may be more or less appropriate for different response variables. It really depends on how variation relates to the response expected mean value. Sometimes, not a single one of these families will provide a good description of variation. In those cases adding random effect, or using a family with zero-inflation or a linear predictor for dispersion can be necessary (not covered in this course).

#### Predictors

Sometimes a count data GLM does not fit well and shows signs of under/over-dispersion because important predictors are missing in the model.

For instance, let's simulate data where a count data "obs" is a function of size and size on the log-scale:
```{r}
set.seed(351188)
fakedata <- tibble(size=rnorm(1000), sex=sample(c(1,2), size=1000, replace = TRUE, prob = c(0.2, 0.8)), 
                   y=1 + 0.5*size + 1.6*sex + rnorm(n = 1000, 0, 0.02), obs=rpois(n = 1000, lambda = exp(y)))

```

We model obs as a function of size only:
```{r}
summary(fake_nbinom1 <- glmmTMB(obs ~ 1 + size, data=fakedata, family = nbinom1)) 
testResiduals(fake_nbinom1)
```

Here we have a case of **under-dispersion**. The model does not fit the data well and over-estimates the amount of noise in the data. In the presence of under-dispersion we expect to find fewer false positives than normal, but also more false negative; that is, our statistical model has less power to detect effects than it should.

Sometimes you can still get an okay model by fitting a different GLM, but adding the missing predictor in the model solves the issue much better:
```{r}
summary(fake_nbinom1_sex <- glmmTMB(obs ~ 1 + size + sex, data=fakedata, family = nbinom1)) 
testResiduals(fake_nbinom1_sex)
```

With the same data, if we fit sex only but not size, we get **over-dispersion**:
```{r}
summary(fake_nbinom1_sex_only <- glmmTMB(obs ~ 1 + sex, data=fakedata, family = nbinom1)) 
testResiduals(fake_nbinom1_sex_only)
```

The issue disappears when we include size in the model:
```{r}
summary(fake_nbinom1_sex <- glmmTMB(obs ~ 1 + size + sex, data=fakedata, family = nbinom1)) 
testResiduals(fake_nbinom1_sex)
```


Time for practice!