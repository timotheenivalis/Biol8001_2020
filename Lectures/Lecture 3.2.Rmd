---
title: "Lecture 3.2 Fitting Models in R Part II"
author: "T. Neeman"
date: "10/07/2020"
output: html_document
---


## Model Parameters

```{r libraries}
library(tidyverse)
```

In the last lecture, we set up a workflow for a statistical analysis of data, using a model framework. We used a simple data example to illustrate the workflow. In this lecture, we'll illustrate how a statistical model is very flexible framework for assessing associations and patterns in our data. 

A model is defined by a set of parameters. The parameters are the signal characteristics of the model. That is, they define the relationship between variables. The data are used to **estimate** the model parameters. As with all estimates, one also estimates the **uncertainty** (SE) of the model parameter estimates. 

The parameters estimated in the seed orchard model were (1) the **mean** in each group, and (2) the **mean difference**. The variation around the means was also estimated, and is of interest to us, especially if we plan to do another study. For example, in a follow-up study, we'll need to estimate the signal:noise ratio; the larger the noise (variation) relative to the signal, the more samples we'll need to distinguish the groups.

We estimate 3 parameters, but there is some redundancy here, since $'meandiff' = mean(SO) - mean(P)$. On the other hand, the mean difference is the parameter of greatest interest to us, because it estimates the association between treatment and outcome. So whilst this model has two parameters, there is more than one way to define the model parameterisation. 

R parameterises the seed orchard model as follows:

* Parameter 1: mean of the reference group. The reference group is the group with factor level 1; in this case group 'SO'. 

* Parameter 2: mean difference between two groups. 

Notice that the mean of the P group is Parameter 1 + Parameter 2. 

The parameter estimates are obtained via the **summary** function applied to the model object.

```{r summary_seed}
seed <- read.csv("../Data/seed orchard data.csv")
seed$seedlot <- factor(seed$seedlot, levels = c("SO", "P"))
model.seed <- lm(dbh ~ seedlot, data = seed)
summary(model.seed)
```

Compare the parameter estimates with the mean estimates from the **emmeans** function

```{r means_seed}
library(emmeans)
emmeans(model.seed, ~seedlot)
```

### Exercise:

(1) What are the model parameters in the dark respiration experiment comparing C3 and C4 plants in Exercise 1, Lecture 3.1? What are the model parameter estimates?

## A model for multiple groups

In Exercise 2, Lecture 3.1, the CSIRO researcher compared yield of 3 wheat varieties. The model associated with this experiment has 3 parameters. One parameterisation is the mean of each variety. But R parameterises the model as:

* Parameter 1: mean of the reference group. We can choose the reference to be the standard variety.

* Parameter 2: mean difference between New and standard (reference) variety.

* Parameter 3: mean difference between NewPLUS and standard (reference) variety.

Notice that $mean(New) = Parameter 1 + Parameter 2$, and $mean(NewPLUS) = Parameter 1 + Parameter 3$. 

When there are no differences amongst the varieties, then $Parameter 2 = Parameter 3 = 0$. The ANOVA table for variety has 2 degrees of freedom, and the associated p-value is a measure of evidence against the hypothesis that both parameters are 0 (no variety effect).

What are the model parameter estimates in the Wheat Variety experiment?

```{r summary_wheat}
wheat <- read.csv("../Data/wheat yield PLUS.csv")
wheat$Variety <- factor(wheat$Variety, levels = c("Standard", "New","NewPLUS"))
model.wheat <- lm(Yield ~ Variety, data = wheat)
summary(model.wheat)
```

### More exercises: 

(2) For the wheat variety experiment, compare the model parameter estimates with the mean estimates obtained from the **emmeans** function.

(3) Peas were grown under 5 different growth media which differed in the type of sugar used. The different types of sugar (including a no sugar control) were:  control, glucose, fructose, g&f and sucrose. The experimenter recorded the lengths of pea sections. Ten pea section lengths were recorded per treatment. 

    (a) fit a model to these data, and report your assessment of how growth media affects pea length.
  
    (b) How many parameters in this model? Which is the reference group for this experiment? Use the summary function to obtain the parameter estimates. 
  
    (c) What is the relationship between the model parameters and the treatment means?
    
    (d) Examine the residual plots? What can you say about the within group variation across the 5 groups?
  
## A model for associations between continuous variables

In Lecture 2, we described a data generating process for plant yield given precipitation. Now let's reverse engineer this, and imagine that we have a set of data collected over 50 years where we've captured annual rain and yield. 

Let's remind ourselves of the data generating process we created in Lecture 2. Then let's take the data we generate, and fit a linear model. We called this process "Machine 1". This time, we'll simulate a large sample of annual_rain, and from there, simulate yield data.

###  Machine 1: Precipitation -> Yield
```{r, fig.height=1.4, fig.width=2, fig.align = 'center'}
set.seed(2020720)
n <-200
annual_rain<-sample(11:100, size=n, replace = TRUE)
yield <- 2 + 5*annual_rain - 0.04* annual_rain^2 + rnorm(n,0,10)
yield_dat<-tibble(annual_rain = annual_rain, yield=yield)
ggplot(yield_dat, aes(annual_rain, yield))+geom_point()
```

Now let's assume we have no information about the data generating process; we have only the data. Given the shape of the data, we propose a model with quadratic terms. This model has 3 parameters: an intercept, a linear term and a quadratic term. We can estimate these parameters using the lm() function:

```{r yield}
model.yield <- lm(yield ~ 1+ annual_rain + I(annual_rain^2), data = yield_dat)
summary(model.yield)
```

Compare the *parameter estimates* with the parameters from the data generating process. If we hadn't generated noise (rnorm(90,0,10)) along with the signal, we would always recover the true parameters. As an exercise, try "increasing" the noise (e.g. rnorm(n,0,20)) or decreasing the sample size (n) and see how this affects the parameter estimates and the standard error ("uncertainty") of the parameter estimates. 

Finally, look at the residuals, and comment upon anything unusual. These these were constructed data, the residuals "should" look approximately normally distributed. 

```{r residual_plots}
par(mfrow = c(2,2))
plot(model.yield)
```

*A few thoughts*: With the precipitation-yield data, we assumed we knew the "form" of the model, ie a quadratic structure. Choosing the form of the model means choosing the parameters to estimate. But this choice often isn't obvious, particularly in ecological research. With lab experiments, the form of the model is usually more obvious, because we've designed the experiment. 

Here is an example of experimental data where we might use a model with a slope and intercept:

Plants respond to external nitrate availability in the soil by altering their root mass ratio (RMR). Under low nitrogen conditions, plants allocate relatively more biomass to the root. Legumes have the ability to form root nodules in symbiosis with $N_2$-fixing rhizobia, and this may impact on the soil nitrogen - RMR relationship characteristic of other plants. 

In this glasshouse experiment, researchers grew legumes under 6 different soil nitrogen conditions: 0.01, 0.1, 1,2, 10, and 100 mM. Seedlings were grown for 4 weeks, then harvested, dried and RMR measured. 

```{r legume_nitrate}
legume<-read.csv("../Data/legume nitrate experiment.csv")
str(legume)
```

We look at the relationship between treatment (nitrate concentration) and response (RMR). Typically, one plots log(concentration) on the x-axis. We present both RMR and log(RMR) as potential response variables. 
```{r legume_plot}
ggplot(legume, aes(x=log10(nitrate), y=RMR))+ geom_beeswarm()
ggplot(legume, aes(x=log10(nitrate), y= log_RMR))+ geom_beeswarm()
```

We consider our model assumptions that the data should be normally distributed around its mean. The RMR distribution is clearly non-normal; it has long tails. On the other hand, the log(RMR) distribution looks more "normal". This is a relatively common phenomenon with biological data, and visualising the data before modelling it will help us choose the most appropriate model.

We have a couple of options regarded how we include nitrate in the model. On the one hand, we have 6 treatments, so nitrate can be a *factor* with 6 levels. On the other hand, we anticipate a dose-response relationship, with decreasing RMR for increasing log(nitrate concentration).

*Option 1:* Treat nitrate concentration as a factor with 6 levels. This model has *6 parameters*, namely the mean log(RMR) for each concentration level. One can estimate the mean differences in log(RMR) between any two nitrate concentration levels, but we might miss the real story, which is how does log(RMR) change for every 10-fold inrease in dose. 

*Option 2:* Treat log10(nitrate concentration) as a continuous variable. The simplest model to consider is log(RMR) = a + b * log10(nitrate). This model has 2 parameters: *a* the intercept, and *b* the slope. *b*  measures the association between log10(nitrate) and log(RMR). For every 10-fold increase in concentration, log(RMR) increases by b units. 

We'll go with option 2, given the data and our research question. 
```{r model_fit}
model.legume<-lm(log_RMR ~ log10(nitrate), data = legume)
anova(model.legume)
summary(model.legume)
```

The anova() function shows us the ANOVA table for the model. The important line is the first line, which indicates strong evidence that log10(nitrate concentration) is associated with log(RMR). We don't yet know the direction of the association. But the ANOVA table provides the inference that indicates that the "signal" we thought we might be seeing is probably "real". 

The summary() function provides the parameter estimates and SE(uncertainty). The two rows under Coefficients are the estimates for *a (Intercept)* and *b (slope)*. The slope estimate *-0.19* is negative, meaning that as log(concentration) increases, log(RMR) decreases. The standard error *0.07* is a measure of our uncertainty around the estimated slope. The t-value *-2.686*  is the ratio of the estimate to the SE: -0.19 / 0.07. I call this the *signal-to-noise* ratio, as it measures the strength of the slope "signal" relative to our uncertainty of the magnitude of the signal. The further t is from 0, the stronger our evidence that the "signal" is real. 

Finally, the p-value is derived from the t-value, and has an easily recognisable interpretation. 

Following our standard workflow, we assess our model assumptions with residual plots.

```{r check_model}
par(mfrow=c(2,2))
plot(model.legume)
```

A summary of the model can be shown together with the data. We need something equivalent to means and standard errors. Instead of the emmeans() function, we'll use the predict() function to get estimated mean log_RMR for a range of nitrate concentrations. The predict() function includes an option for 95% confidence intervals around the estimated means. The confidence interval is mean +- 1.96*SE.
```{r model_summary, fig.height = 3, fig.width = 2, fig.align = 'center'}
 newdat<-data.frame(nitrate=c(0.01, .1, 1, 2, 10,100))
 
predict1<-predict(check, newdat, interval = "confidence") %>%
   as_tibble() %>%
   mutate(nitrate=newdat$nitrate, log_RMR=fit)

str(predict1)

ggplot(legume, aes(x=log10(nitrate), y=log_RMR))+
  geom_point(col="darkgreen")+
  geom_line(data=predict1, aes(x=log10(nitrate), y=fit))+
  geom_ribbon(data=predict1, aes(x=log10(nitrate), ymin=lwr, ymax=upr), alpha=0.2)+
  ylim(c(-8,-2))+
  theme_classic()
  
```

## Exercises

(1) A clinical research group has collected data on breast density in adult women. They are  interested in the relationship between breast density and age and body mass index. Is there a loss of breast density as women age? Does BMI have an impact on breast density? These data are in Breast cancer density.csv. 

Set up a workflow to address these questions. 

*hint:* To help you to visualise these associations in ggplot(), you can use the geom_smooth() function. 

How many parameters in the model: density ~ AGE?
What can you infer from fitting this model?
Comment on the residual plots for the model density~AGE.

Same questions for the model: density ~ BMI.

Consider the model:  density ~ AGE + BMI. 
What can you infer from fitting this model?
Interpret the parameter estimates.